density modeling = discovering structures underlying data

naive "unsupervised learning": summary stats(mean, var, ...do people naturally do that?); 
myQ: are we mimicing anything that the brain is doing? Or is it in anyways plausible/generates any intuition? not really
It's more a ML problem and criteria is how good you can learn natural images...here is looking into the "prior of latent variable" i.e. let algorithm generate natural images

objective function: loglikelihood P(data|model); or resample from the new model(then look into what?); or resample from the new model, see LL of resampled points -- or even the distribution of LL, then finally compare the LL of P(data|model).

however, if you want more flexibility of your fitted model => transform from a simpler latent model, and each observed data is tranformed from a latent variable. 
=> you need to optimize the transformation parameters.
way1: integrate over all possible latent variables z, and maximize the posterior -- computationally costly
way2: try to maximize z and transformation pars at the same time
way3: based on way1, but since posterior for z(the thing you are integrating) is complicated, you can propose an alternative posterior, say q(z|x). Then put it into integral, make the Int an expectance under q(z|x). Then take log and using Jensen inequality to break apart everything
=>optimizing both the parameters w and q(z|x).

altho as Eero pointed out: since your goal is maximizing transformation pars(w), finding a maxmimum lower bound don't necessarily mean the corresponding location of that maximum(w) is closer to the real w.

Then to further simplify the optimization, add "automatic encoder" for 
